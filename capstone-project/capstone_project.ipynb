{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# ETL Pipeline for Immigration and Average Temperature\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The analyst team are analysing seasonal pattern of immigration from one country to another, following destination temperature. The project needs to build an ETL pipeline to form an analytics database on immigration events and the temperature of their destintion.\n",
    "\n",
    "#### Project Datasets:\n",
    "Project used I94 immigration data, Global land and temperature by City and global I94 port data. Accessing the data:\n",
    "* **Immigration data**: `../../data/18-83510-I94-Data-2016/`\n",
    "* **Temperature data**: `../../data2/GlobalLandTemperaturesByCity.csv`\n",
    "* **Valid i94 data**: `i94port_valid.txt` extracted from the file I94_SAS_Labels_Descriptions\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "MAX_MEMORY = \"1g\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "            .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "            .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "            .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "### Scope of the Project:\n",
    "- Use Spark to load the data into the dataframes.\n",
    "- Cleanse the data for Global temperature.\n",
    "- Create a dataframe containing fields City, Country, Port from i94_valid.txt file.\n",
    "- Create a dataframe from temperature data: City, Country, Port,  Year, Month, Average Temperature\n",
    "- Cleanse the data for i94 immigration\n",
    "- Create a dataframe for i94 immigration: Port, Year, Month, Count\n",
    "- Create a fact table from immigration and temprature dataframes: Port, Month, Average Temperature, Count\n",
    "\n",
    "### Description and Gather Data:\n",
    "I94 immigration [data](https://travel.trade.gov/research/reports/i94/historical/2016.html) comes from the US National Tourism and Trade Office. Data is provided in SAS7BDAT format. Some relevant attributes include:\n",
    "\n",
    "* **i94yr**: 4 digit year\n",
    "* **i94mon**: numeric month\n",
    "* **i94port**: 3 character code of destination USA city\n",
    "* **arrdate**: arrival date in the USA\n",
    "\n",
    "The temperature [data](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data) comes from Kaggle. Data is provided in csv format. Some relevant attributes include:\n",
    "\n",
    "* **AverageTemperature** = average temperature\n",
    "* **City**: city name\n",
    "* **Country**: country name\n",
    "* **Latitude**: latitude\n",
    "* **Longitude**: longitude\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###  Load Temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|                 dt| AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+-------------------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01 00:00:00|              6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01 00:00:00|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01 00:00:00|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01 00:00:00|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01 00:00:00|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-04-01 00:00:00| 5.7879999999999985|           3.6239999999999997|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-05-01 00:00:00|             10.644|           1.2830000000000001|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-06-01 00:00:00| 14.050999999999998|                        1.347|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-07-01 00:00:00|             16.082|                        1.396|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-08-01 00:00:00|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-09-01 00:00:00| 12.780999999999999|                        1.454|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-10-01 00:00:00|               7.95|                         1.63|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-11-01 00:00:00|  4.638999999999999|           1.3019999999999998|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-12-01 00:00:00|0.12199999999999987|                        1.756|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-01-01 00:00:00|-1.3330000000000002|                        1.642|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-02-01 00:00:00|             -2.732|                        1.358|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-03-01 00:00:00|              0.129|                        1.088|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-04-01 00:00:00|              4.042|                        1.138|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-05-01 00:00:00|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-06-01 00:00:00|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+-------------------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read csv Global Land Temperatures by City\n",
    "global_temp_df = spark.read \\\n",
    "                .options(header=True, delimiter=',', inferSchema=True, dateFormat=\"yyyy-MM-dd\") \\\n",
    "                .csv('../../data2/GlobalLandTemperaturesByCity.csv')\n",
    "\n",
    "global_temp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###  2.1: Data cleaning for Temperature data\n",
    "There are some City, Country combination for which there exists multiple locations (Eg. Springfield, United States). In order to distinguish between them, we will need to take Lat/Long into consideration. To simplify it, we are skipping the entries that belong to these City, Country combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----+\n",
      "|       City|      Country|count|\n",
      "+-----------+-------------+-----+\n",
      "|     Jining|        China|    2|\n",
      "|  Arlington|United States|    2|\n",
      "|Springfield|United States|    3|\n",
      "|     Haikou|        China|    2|\n",
      "|   Pasadena|United States|    2|\n",
      "|  Rongcheng|        China|    3|\n",
      "|    Luoyang|        China|    2|\n",
      "|     Yichun|        China|    2|\n",
      "|   Richmond|United States|    2|\n",
      "|   Haicheng|        China|    2|\n",
      "|      Depok|    Indonesia|    2|\n",
      "|      Taman|    Indonesia|    2|\n",
      "|  Yingcheng|        China|    2|\n",
      "|     Aurora|United States|    2|\n",
      "|     Peoria|United States|    2|\n",
      "|     Suzhou|        China|    2|\n",
      "|   Columbus|United States|    2|\n",
      "|   Glendale|United States|    2|\n",
      "+-----------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cleanup records with no mention of average temperature\n",
    "global_temp_df = global_temp_df.filter(global_temp_df.AverageTemperature != 'NaN')\n",
    "\n",
    "# List of city/country combination for which there are multiple locations named that.\n",
    "duplicate_df = global_temp_df \\\n",
    "    .select(\"City\", \"Country\", \"Latitude\", \"Longitude\") \\\n",
    "    .distinct() \\\n",
    "    .groupBy(\"City\", \"Country\") \\\n",
    "    .count() \\\n",
    "    .filter(\"count > 1\")\n",
    "duplicate_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|City|Country|count|\n",
      "+----+-------+-----+\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove those common city/country name from the dataframe\n",
    "temp_df = global_temp_df.join(duplicate_df, [\"City\", \"Country\"], \"leftanti\")\n",
    "\n",
    "# Check if there are any duplicates\n",
    "temp_df \\\n",
    "    .select(\"City\", \"Country\", \"Latitude\", \"Longitude\") \\\n",
    "    .distinct() \\\n",
    "    .groupBy(\"City\", \"Country\") \\\n",
    "    .count() \\\n",
    "    .filter(\"count > 1\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###  2.2: Create a dataframe containing the fields - City, Country, Port from i94port_valid.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_city_country_tuple(location):\n",
    "    '''\n",
    "    Given a string, return a tuple containing city and country. Eg.\n",
    "    \n",
    "    Input: ANACORTES, WA\n",
    "    Output: (ANACORTES, UNITED STATES)\n",
    "    \n",
    "    Input: DUBLIN, IRELAND\n",
    "    Output: (DUBLIN, IRELAND)\n",
    "    '''\n",
    "    \n",
    "    vals = location.rsplit(',', 1)\n",
    "    if len(vals) != 2:\n",
    "        return\n",
    "    \n",
    "    city = vals[0].strip();\n",
    "    country = vals[1].strip();\n",
    "    \n",
    "    if (len(country) == 2):\n",
    "        country = \"UNITED STATES\"\n",
    "        \n",
    "    return (city, country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "reg_str = re.compile(r'\\'(.*)\\'.*\\'(.*)\\'')\n",
    "port_locations = []\n",
    "\n",
    "'''\n",
    "Uses i94port_valid.txt to create a dictionary of (city, country) -> port.\n",
    "This will be used below to get the port based on city and country.\n",
    "'''\n",
    "with open('i94port_valid.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.upper()\n",
    "        matched_str = reg_str.search(line)\n",
    "        \n",
    "        port = matched_str.group(1).strip()\n",
    "        location = matched_str.group(2).strip()\n",
    "        \n",
    "        tup = get_city_country_tuple(location);\n",
    "        \n",
    "        if tup:\n",
    "            city = tup[0]\n",
    "            country = tup[1]\n",
    "            port_locations.append((city, country, port))\n",
    "    # print(port_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Port: string (nullable = true)\n",
      "\n",
      "+------------------------+-------------+----+\n",
      "|City                    |Country      |Port|\n",
      "+------------------------+-------------+----+\n",
      "|ALCAN                   |UNITED STATES|ALC |\n",
      "|ANCHORAGE               |UNITED STATES|ANC |\n",
      "|BAKER AAF - BAKER ISLAND|UNITED STATES|BAR |\n",
      "|DALTONS CACHE           |UNITED STATES|DAC |\n",
      "|DEW STATION PT LAY DEW  |UNITED STATES|PIZ |\n",
      "|DUTCH HARBOR            |UNITED STATES|DTH |\n",
      "|EAGLE                   |UNITED STATES|EGL |\n",
      "|FAIRBANKS               |UNITED STATES|FRB |\n",
      "|HOMER                   |UNITED STATES|HOM |\n",
      "|HYDER                   |UNITED STATES|HYD |\n",
      "|JUNEAU                  |UNITED STATES|JUN |\n",
      "|KETCHIKAN               |UNITED STATES|5KE |\n",
      "|KETCHIKAN               |UNITED STATES|KET |\n",
      "|MOSES POINT INTERMEDIATE|UNITED STATES|MOS |\n",
      "|NIKISKI                 |UNITED STATES|NIK |\n",
      "|NOM                     |UNITED STATES|NOM |\n",
      "|POKER CREEK             |UNITED STATES|PKC |\n",
      "|PORT LIONS SPB          |UNITED STATES|ORI |\n",
      "|SKAGWAY                 |UNITED STATES|SKA |\n",
      "|ST. PAUL ISLAND         |UNITED STATES|SNP |\n",
      "+------------------------+-------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe of valid ports from i94_valid.txt file.\n",
    "port_location_schema = [\"City\", \"Country\", \"Port\"]\n",
    "port_location_df = spark.createDataFrame(data=port_locations, schema = port_location_schema)\n",
    "\n",
    "port_location_df.printSchema()\n",
    "port_location_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2.3: Create DF for i94 immigration\n",
    "Extract fields (Year, Month, Port, Count) for arrivals from i94 immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def filter_i94_data(file, spark_session):\n",
    "    '''\n",
    "    Function filters out only valid i94port records from sas7bdat file.\n",
    "    \n",
    "    file : I94 immigration data file,\n",
    "    spark_session : Spark session\n",
    "    \n",
    "    Return: I94 immigration dataframe of only valid i94port data\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Read I94 data from the file and load into spark dataframe\n",
    "    i94_immigration_df = spark_session.read.format('com.github.saurfang.sas.spark').load(file)\n",
    "\n",
    "    # Remove all the invalid records from the i94port column\n",
    "    # i94_immigration_df = i94_immigration_df.filter(i94_immigration_df.i94port.isin(list(i94port_valid_dict.keys())))\n",
    "    \n",
    "    i94_immigration_df = i94_immigration_df \\\n",
    "        .filter(i94_immigration_df.arrdate != 'NaN') \\\n",
    "        .select(\n",
    "            col(\"i94port\").alias(\"Port\"),\n",
    "            col(\"i94yr\").cast(IntegerType()).alias(\"Year\"),\n",
    "            col(\"i94mon\").cast(IntegerType()).alias(\"Month\")\n",
    "        ) \\\n",
    "        .groupBy([\"Port\", \"Year\", \"Month\"]) \\\n",
    "        .count() \\\n",
    "        .orderBy([\"Port\", \"Year\", \"Month\"])\n",
    "\n",
    "    return i94_immigration_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i94_jul16_sub.sas7bdat',\n",
       " 'i94_jun16_sub.sas7bdat',\n",
       " 'i94_mar16_sub.sas7bdat',\n",
       " 'i94_apr16_sub.sas7bdat',\n",
       " 'i94_may16_sub.sas7bdat',\n",
       " 'i94_oct16_sub.sas7bdat',\n",
       " 'i94_jan16_sub.sas7bdat',\n",
       " 'i94_feb16_sub.sas7bdat',\n",
       " 'i94_aug16_sub.sas7bdat',\n",
       " 'i94_nov16_sub.sas7bdat',\n",
       " 'i94_sep16_sub.sas7bdat',\n",
       " 'i94_dec16_sub.sas7bdat']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "files = os.listdir('../../data/18-83510-I94-Data-2016/')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a thread for each file to process the data.\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "def process_file(file_name):\n",
    "    '''\n",
    "        Process a file\n",
    "        Input: file_name\n",
    "        Return: i94 immigration dataframe\n",
    "    '''\n",
    "    \n",
    "    file_path = '../../data/18-83510-I94-Data-2016/' + file_name\n",
    "    return filter_i94_data(file_path, spark)\n",
    "\n",
    "pool = ThreadPool(len(files))\n",
    "df_coll = pool.map(lambda file_name: process_file(file_name), files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+-----+\n",
      "|Port|Year|Month|count|\n",
      "+----+----+-----+-----+\n",
      "| 48Y|2016|    7|    2|\n",
      "| 5KE|2016|    3|    2|\n",
      "| 5KE|2016|    4|    3|\n",
      "| 5KE|2016|    6|   18|\n",
      "| 5KE|2016|    7|   25|\n",
      "| 5KE|2016|    8|    9|\n",
      "| 5KE|2016|    9|    1|\n",
      "| 5T6|2016|    1|   12|\n",
      "| 5T6|2016|    2|   18|\n",
      "| 5T6|2016|    3|   20|\n",
      "| 5T6|2016|    4|    4|\n",
      "| 5T6|2016|    5|   11|\n",
      "| 5T6|2016|    6|   12|\n",
      "| 5T6|2016|    7|   18|\n",
      "| 5T6|2016|    8|   12|\n",
      "| 5T6|2016|    9|   11|\n",
      "| 5T6|2016|   10|   16|\n",
      "| 5T6|2016|   11|   14|\n",
      "| 5T6|2016|   12|   13|\n",
      "| 74S|2016|    5|    1|\n",
      "+----+----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Merge all the immigration dataframe created by an individual file \n",
    "# and create a new merged dimension immigration dataframe\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "dim_immigration_df = reduce(lambda df1, df2: df1.union(df2.select(df1.columns)), df_coll)\n",
    "dim_immigration_df = dim_immigration_df.orderBy([\"Port\", \"Year\", \"Month\"])\n",
    "\n",
    "dim_immigration_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 3: Define the Data Model\n",
    "\n",
    "![alt text](ER-diagram.png \"Title\")\n",
    "\n",
    "### Dimension Tables\n",
    "* **port_location_df** - global I94 port location  \n",
    "    *City, Country, Port*\n",
    "    \n",
    "* **dim_temp_df** - global average temperature  \n",
    "    *City, Country, Year, Month, AverageTemperature*\n",
    "\n",
    "* **dim_immigration_df** - I94 immigration only for arrivals  \n",
    "    *Port, Year, Month, count*\n",
    "\n",
    "### Fact Table\n",
    "* **avg_temp_fact_df** - Monthly records of arrivals in the country/city with average temperature \n",
    "    *Port, Month, AverageTemperature, Count*\n",
    "    \n",
    "### Snowflake Schema based model:\n",
    "With this project, I wanted to find the effect of temperature on the number of passengers arriving at a port. The data we had were:\n",
    "1. Dim table #1 (port_location): The mapping of port to the country/city it is located in. It is created from i94port_valid.txt file where only valid i94port location details are extracted. \n",
    "2. Dim table #2 (dim_temp): The average monthly temperature of a country/city for a given month and year. It is extracted from global temperature data.\n",
    "3. Dim table #3 (dim_immigration): The count of passengers arriving at (and departing from) a port in a given year/month. It is extracted from I94 immigration data.\n",
    "\n",
    "These had some commonalities using which we could obtain the desired outcome. I chose to do it in two steps:\n",
    "1. Create another dim table (avg_temp) by joining port_location (Dim table #1) and dim_temp (Dim table #2) based on the fields *city* and *country*. This would thus provide the average monthly temperature at a port for a given year and month.\n",
    "2. Join the dim table from above with dim_immigration (Dim table #3) using port, year and month to get the desired outcome. \n",
    "\n",
    "To the reason behind choosing Snowkflake Schema:\n",
    "* We needed to create an intermediate dim_table.\n",
    "* Snowflake can avoid redundant data which makes it easier to maintain.\n",
    "* Snowflake schema leads to reduced data redundancy and consumes lesser disk space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 4: Run Pipelines to Model the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write immigration dimension table to parquet files partitioned by port\n",
    "dim_immigration_df.write.mode(\"append\").partitionBy(\"port\").parquet(\"results/dim_immigration.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.1: Create a dataframe containing: Year, Month, City, Country, Average Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      "\n",
      "+--------+-------+----+-----+------------------+\n",
      "|    City|Country|Year|Month|AverageTemperature|\n",
      "+--------+-------+----+-----+------------------+\n",
      "|A CORUÑA|  SPAIN|1743|   11|10.779000000000002|\n",
      "|A CORUÑA|  SPAIN|1744|    4|            13.325|\n",
      "|A CORUÑA|  SPAIN|1744|    5|              12.9|\n",
      "|A CORUÑA|  SPAIN|1744|    6|             16.41|\n",
      "|A CORUÑA|  SPAIN|1744|    7|            17.992|\n",
      "|A CORUÑA|  SPAIN|1744|    9|16.067999999999998|\n",
      "|A CORUÑA|  SPAIN|1744|   10|12.904000000000002|\n",
      "|A CORUÑA|  SPAIN|1744|   11|            11.028|\n",
      "|A CORUÑA|  SPAIN|1744|   12|             8.798|\n",
      "|A CORUÑA|  SPAIN|1745|    1| 7.651999999999999|\n",
      "+--------+-------+----+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, upper, col\n",
    "\n",
    "# Create a dimension dataframe from the temperature dataframe\n",
    "\n",
    "dim_temp_df = temp_df \\\n",
    "    .select(\n",
    "        year(\"dt\").alias('Year'), \n",
    "        month(\"dt\").alias('Month'),\n",
    "        upper(col(\"City\")).alias('City'),\n",
    "        upper(col(\"Country\")).alias('Country'),\n",
    "        \"AverageTemperature\"\n",
    "    ) \\\n",
    "    .groupBy(\"City\", \"Country\", \"Year\", \"Month\") \\\n",
    "    .avg(\"AverageTemperature\") \\\n",
    "    .withColumnRenamed('avg(AverageTemperature)', 'AverageTemperature') \\\n",
    "    .orderBy(\"City\", \"Country\", \"Year\", \"Month\")\n",
    "\n",
    "dim_temp_df.printSchema()\n",
    "dim_temp_df.show(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Port: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      "\n",
      "+-------+-------------+----+----+-----+------------------+\n",
      "|   City|      Country|Port|Year|Month|AverageTemperature|\n",
      "+-------+-------------+----+----+-----+------------------+\n",
      "|BUFFALO|UNITED STATES| BUF|1743|   11|1.6879999999999995|\n",
      "|BUFFALO|UNITED STATES| BUF|1744|    4| 7.699999999999998|\n",
      "|BUFFALO|UNITED STATES| BUF|1744|    5|            13.613|\n",
      "|BUFFALO|UNITED STATES| BUF|1744|    6|            19.437|\n",
      "|BUFFALO|UNITED STATES| BUF|1744|    7|            20.682|\n",
      "|BUFFALO|UNITED STATES| BUF|1744|    9|            13.662|\n",
      "|BUFFALO|UNITED STATES| BUF|1744|   10| 7.449000000000002|\n",
      "|BUFFALO|UNITED STATES| BUF|1744|   11|1.7279999999999998|\n",
      "|BUFFALO|UNITED STATES| BUF|1744|   12|            -4.157|\n",
      "|BUFFALO|UNITED STATES| BUF|1745|    1|            -4.633|\n",
      "+-------+-------------+----+----+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Port column is added in dim_tem_df on the basis of City and Country\n",
    "avg_temp_df = port_location_df.join(dim_temp_df, ['City','Country'])\n",
    "\n",
    "avg_temp_df.printSchema()\n",
    "avg_temp_df.show(n = 10)\n",
    "\n",
    "# Write average temperature dimension table to parquet files partitioned by port\n",
    "avg_temp_df.write.mode(\"append\").partitionBy(\"port\").parquet(\"results/dim_temperature.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.2 Create a fact table from immigration and temprature dataframes: Port, Month, Average Temperature, Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Caveat\n",
    "The Global temperatures data is available only upto 2013. But the immigration data is available only for year 2016. So we will not be able to perform a join on the two DFs.\n",
    "\n",
    "### Approach\n",
    "We will:\n",
    "1. only take the temperatures for the year 2010 into consideration\n",
    "2. Use the monthly immigration data for the year 2016\n",
    "3. Create a fact table with cols: [Port, Month, AverageTemperature, Count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+\n",
      "|Port|Month|AverageTemperature|\n",
      "+----+-----+------------------+\n",
      "| ABQ|    1|            -0.581|\n",
      "| ABQ|    2|             1.537|\n",
      "| ABQ|    3| 5.497999999999998|\n",
      "| ABQ|    4|            11.025|\n",
      "| ABQ|    5|            15.283|\n",
      "| ABQ|    6|            22.448|\n",
      "| ABQ|    7|             23.39|\n",
      "| ABQ|    8|22.215999999999998|\n",
      "| ABQ|    9|            20.075|\n",
      "| ABQ|   10|             12.93|\n",
      "| ABQ|   11|4.1770000000000005|\n",
      "| ABQ|   12|             3.202|\n",
      "| AKR|    1|              -3.7|\n",
      "| AKR|    2|            -3.411|\n",
      "| AKR|    3| 5.372000000000001|\n",
      "| AKR|    4|            12.232|\n",
      "| AKR|    5|17.128999999999998|\n",
      "| AKR|    6|            21.936|\n",
      "| AKR|    7|24.203000000000003|\n",
      "| AKR|    8|            23.502|\n",
      "+----+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, upper, col\n",
    "\n",
    "# only take the temperatures for the year 2010 into consideration\n",
    "\n",
    "monthly_avg_temp_df = avg_temp_df \\\n",
    "    .filter(avg_temp_df.Year == 2010) \\\n",
    "    .select(\n",
    "        \"Port\",\n",
    "        \"Month\",\n",
    "        \"AverageTemperature\"\n",
    "    ) \\\n",
    "    .orderBy(\"Port\", \"Month\")\n",
    "\n",
    "monthly_avg_temp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Use the monthly immigration data for the year 2016\n",
    "The monthly immigration data has data for 2016 only. So no additional work is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create a fact table with cols: [Port, Month, AverageTemperature, Count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------------------+-----+\n",
      "|Port|Month| AverageTemperature|Count|\n",
      "+----+-----+-------------------+-----+\n",
      "| ABQ|    1|             -0.581|   16|\n",
      "| ABQ|    2|              1.537|    3|\n",
      "| ABQ|    3|  5.497999999999998|    1|\n",
      "| ABQ|    4|             11.025|    3|\n",
      "| ABQ|    6|             22.448|    1|\n",
      "| ABQ|    7|              23.39|    9|\n",
      "| ABQ|    8| 22.215999999999998|    8|\n",
      "| ABQ|    9|             20.075|    1|\n",
      "| ABQ|   10|              12.93|    7|\n",
      "| ABQ|   11| 4.1770000000000005|    5|\n",
      "| ABQ|   12|              3.202|    7|\n",
      "| ANC|    1|            -13.011|  109|\n",
      "| ANC|    2|             -7.597|  124|\n",
      "| ANC|    3|              -7.39|   96|\n",
      "| ANC|    4|-1.2220000000000002|   91|\n",
      "| ANC|    5| 6.3629999999999995| 1897|\n",
      "| ANC|    6|              9.654| 4341|\n",
      "| ANC|    7|             10.757| 6555|\n",
      "| ANC|    8|             10.384| 5736|\n",
      "| ANC|    9|  6.252000000000001| 2805|\n",
      "+----+-----+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "fact_df = dim_immigration_df.join(monthly_avg_temp_df, ['Port', 'Month'])\n",
    "\n",
    "avg_temp_fact_df = fact_df.select(\n",
    "        \"Port\",\n",
    "        \"Month\",\n",
    "        \"AverageTemperature\",\n",
    "        col(\"count\").alias(\"Count\")\n",
    "    ) \\\n",
    "    .orderBy([\"Port\", \"Month\"])\n",
    "\n",
    "avg_temp_fact_df.show()\n",
    "\n",
    "# Write fact table to parquet files partitioned by port\n",
    "avg_temp_fact_df.write.mode(\"append\").partitionBy(\"port\").parquet(\"results/avg_temp_fact.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.2: Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records for average temperature table = 8146263 Data quality check passed. Hurray!!\n",
      "Number of records for immigration table = 3649 Data quality check passed. Hurray!!\n",
      "Number of records for fact table = 939 Data quality check passed. Hurray!!\n"
     ]
    }
   ],
   "source": [
    "def quality_check(df, df_name):\n",
    "    '''\n",
    "    df: dataframe\n",
    "    \n",
    "    df_name: dataframe name (String)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    output = df.count()\n",
    "    if output == 0:\n",
    "        print(\"No records found in {}!! Data uality check failed. UGH!!!\".format(df_name))\n",
    "    else:\n",
    "        print(\"Number of records for {} = {} Data quality check passed. Hurray!!\".format(df_name, output))\n",
    "\n",
    "# Check for data quality\n",
    "quality_check(dim_temp_df, \"average temperature table\")\n",
    "quality_check(dim_immigration_df, \"immigration table\")\n",
    "quality_check(avg_temp_fact_df, \"fact table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.3 Data dictionary \n",
    "\n",
    "Temperature Dimension dataframe - data dictionary\n",
    "\n",
    "* `City` = city name\n",
    "* `Country` = country name\n",
    "* `Year` = 4 digit year\n",
    "* `Month` = numeric month\n",
    "* `port` = 3 character code of destination city\n",
    "* `AverageTemperature` = average temperature\n",
    "\n",
    "I94 immigration dataframe - data dictionary\n",
    "* `Year` = 4 digit year\n",
    "* `Month` = numeric month\n",
    "* `Port` = 3 character code of destination city\n",
    "* `count` = number of arrivals in the i94 port\n",
    "\n",
    "Fact table - data dictionay\n",
    "* `Port` = 3 character code of destination city\n",
    "* `Month` = numeric month\n",
    "* `AverageTemperature` = average temperature of destination city\n",
    "* `count` = number of arrivals in the i94 port"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 5: Complete Project Write Up\n",
    "\n",
    "**Clearly state the rationale for the choice of tools and technologies for the project.**\n",
    "   * Spark was chosen for the project as it can process in multiple threads and store data in-memory which speed up data processing. Also, it can handle any kind of data format like binary data (SAS), csv, json and more.\n",
    "    \n",
    "**Propose how often the data should be updated and why.**\n",
    "   * The temperature data should be updated daily and immigration data monthly. Currently they are updated in that often.\n",
    "    \n",
    "**Write a description of how you would approach the problem differently under the following scenarios:**\n",
    " * **The data was increased by 100x.**\n",
    "     * Spark can still handle the volume but we need to increase the number of nodes in our cluster. Also, local cluster manager needs to be moved to yarn.\n",
    " \n",
    " * **The data populates a dashboard that must be updated on a daily basis by 7am every day.**\n",
    "     * I will schedule and ran the ETL pipeline in Apache Airflow.\n",
    "     \n",
    " * **The database needed to be accessed by 100+ people.**\n",
    "     * I will use Amazon Redshift to access 100+ people as it is fast in performance, auto-scalable, and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
